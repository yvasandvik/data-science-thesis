{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Random Forest Model\n\nRF model to be used for predicting protein coding genes in DNA sequences.\n\n**Random Search with CV**\n- bootstrap': False\n- max_depth': 20\n- max_features': 'auto'\n- min_samples_leaf': 1\n- min_samples_split': 2\n- n_estimators': 1000\n \n**Grid Search with CV**\n- bootstrap': False\n- max_depth': 40\n- max_features': 3\n- min_samples_leaf': 1\n- min_samples_split': 2\n- n_estimators': 2000\n\nBEST MODEL:\n- Random Search with CV - BUT\n- n_estimators': 200\n- n_features: 200","metadata":{}},{"cell_type":"markdown","source":"## Import packages and choose data","metadata":{}},{"cell_type":"code","source":"# Importing data\nimport pandas as pd\nimport time\n\n# Preprocessing and encoding variables\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Visualising feature importance and making plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import export_graphviz\nimport pydot\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:59:03.416451Z","iopub.execute_input":"2022-02-14T13:59:03.417330Z","iopub.status.idle":"2022-02-14T13:59:04.074114Z","shell.execute_reply.started":"2022-02-14T13:59:03.417179Z","shell.execute_reply":"2022-02-14T13:59:04.073379Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Prediction of genomes from same species but different strain","metadata":{}},{"cell_type":"code","source":"# Both genomes from E.coli species, but different strain\nG11 = pd.read_csv('../input/genomespart2/G11.features.csv').iloc[:, 1:]\nG12 = pd.read_csv('../input/genomespart2/G12.features.csv').iloc[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T13:59:07.508480Z","iopub.execute_input":"2022-02-14T13:59:07.509171Z","iopub.status.idle":"2022-02-14T14:00:28.920550Z","shell.execute_reply.started":"2022-02-14T13:59:07.509131Z","shell.execute_reply":"2022-02-14T14:00:28.919495Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"frames = [G11, G12]\ne_coli = pd.concat(frames)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:28.922469Z","iopub.execute_input":"2022-02-14T14:00:28.922701Z","iopub.status.idle":"2022-02-14T14:00:31.823933Z","shell.execute_reply.started":"2022-02-14T14:00:28.922675Z","shell.execute_reply":"2022-02-14T14:00:31.823028Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"e_coli.tail(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:31.825428Z","iopub.execute_input":"2022-02-14T14:00:31.825671Z","iopub.status.idle":"2022-02-14T14:00:31.868688Z","shell.execute_reply.started":"2022-02-14T14:00:31.825641Z","shell.execute_reply":"2022-02-14T14:00:31.867797Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Prediction of genomes from different species ","metadata":{}},{"cell_type":"code","source":"# Average genomes from different species\nG7 = pd.read_csv('../input/genomes-part1/G7.features.csv').iloc[:, 1:]\nG13 = pd.read_csv('../input/genomespart2/G13.features.csv').iloc[:, 1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [G11, G12]\ndiff = pd.concat(frames)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff.tail(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combining 5 genomes","metadata":{}},{"cell_type":"code","source":"G1 = pd.read_csv('../input/genomes-part1/G1.features.csv').iloc[:, 1:]\nG2 = pd.read_csv('../input/genomespart2/G2.features.csv').iloc[:, 1:]\nG3 = pd.read_csv('../input/genomespart2/G3.features.csv').iloc[:, 1:]\nG4 = pd.read_csv('../input/genomespart2/G4.features.csv').iloc[:, 1:]\nG5 = pd.read_csv('../input/genomespart2/G5.features.csv').iloc[:, 1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [G1, G2, G3, G4, G5]\n\ndf = pd.concat(frames)\ndf = df.iloc[: , 1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"G = e_coli\nprint('The shape of our dataframe is:', G.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:31.870484Z","iopub.execute_input":"2022-02-14T14:00:31.870716Z","iopub.status.idle":"2022-02-14T14:00:31.875663Z","shell.execute_reply.started":"2022-02-14T14:00:31.870690Z","shell.execute_reply":"2022-02-14T14:00:31.874822Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"G = G.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:31.876915Z","iopub.execute_input":"2022-02-14T14:00:31.877141Z","iopub.status.idle":"2022-02-14T14:00:36.666192Z","shell.execute_reply.started":"2022-02-14T14:00:31.877112Z","shell.execute_reply":"2022-02-14T14:00:36.661440Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print('The shape of our dataframe is:', G.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:36.671834Z","iopub.execute_input":"2022-02-14T14:00:36.672212Z","iopub.status.idle":"2022-02-14T14:00:36.678694Z","shell.execute_reply.started":"2022-02-14T14:00:36.672148Z","shell.execute_reply":"2022-02-14T14:00:36.677704Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Separate out unwanted data based on column 'Dataset'","metadata":{}},{"cell_type":"code","source":"G = G.loc[G['Dataset'] == 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The shape of our dataframe is:', G.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing / Data preparation\n\n1. One-hot encoded categorical variables\n2. Split data into features and labels\n3. Convert to arrays\n4. Split data into training and testing sets","metadata":{}},{"cell_type":"markdown","source":"### Encoding target values","metadata":{}},{"cell_type":"code","source":"def encode_feature(array):\n    \"\"\" Encode a categorical array into a number array\n    \n    :param array: array to be encoded\n    :return: numerical array\n    \"\"\"\n  \n    encoder = preprocessing.LabelEncoder()\n    encoder.fit(array)\n    return encoder.transform(array)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:36.680436Z","iopub.execute_input":"2022-02-14T14:00:36.680779Z","iopub.status.idle":"2022-02-14T14:00:36.693839Z","shell.execute_reply.started":"2022-02-14T14:00:36.680743Z","shell.execute_reply":"2022-02-14T14:00:36.692449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class_names = ['CDS', 'LORF']\ntargets = G[\"Type\"].values\nprint(targets)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:36.695383Z","iopub.execute_input":"2022-02-14T14:00:36.696042Z","iopub.status.idle":"2022-02-14T14:00:36.709895Z","shell.execute_reply.started":"2022-02-14T14:00:36.695956Z","shell.execute_reply":"2022-02-14T14:00:36.708821Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"targets = encode_feature(targets)\nprint(targets)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:36.711115Z","iopub.execute_input":"2022-02-14T14:00:36.711721Z","iopub.status.idle":"2022-02-14T14:00:36.724806Z","shell.execute_reply.started":"2022-02-14T14:00:36.711677Z","shell.execute_reply":"2022-02-14T14:00:36.723979Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print('The shape of our dataframe is:', G.shape)\nprint('Rows:', G.shape[0])\nprint('Columns:', G.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:36.728395Z","iopub.execute_input":"2022-02-14T14:00:36.728660Z","iopub.status.idle":"2022-02-14T14:00:36.736242Z","shell.execute_reply.started":"2022-02-14T14:00:36.728630Z","shell.execute_reply":"2022-02-14T14:00:36.735240Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Selecting features and targets and converting data to arrays","metadata":{}},{"cell_type":"code","source":"# Labels are the values we want to predict\nlabels = targets\n\n# Remove the labels from the features -> axis 1 refers to the columns\nfeatures = G.drop(['Type','Genome', 'Dataset'], axis = 1)\n\n# Saving feature names as list for later use\nfeature_names = list(features.columns)\n\n# Convert to numpy array\nfeatures = np.array(features)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:36.738010Z","iopub.execute_input":"2022-02-14T14:00:36.738752Z","iopub.status.idle":"2022-02-14T14:00:40.682187Z","shell.execute_reply.started":"2022-02-14T14:00:36.738707Z","shell.execute_reply":"2022-02-14T14:00:40.681305Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print('The shape of our features are:', features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:40.683803Z","iopub.execute_input":"2022-02-14T14:00:40.684015Z","iopub.status.idle":"2022-02-14T14:00:40.689485Z","shell.execute_reply.started":"2022-02-14T14:00:40.683989Z","shell.execute_reply":"2022-02-14T14:00:40.688609Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Split into training and testing sets","metadata":{}},{"cell_type":"code","source":"# Split the data into training and testing sets -> x = features and y = labels/targets\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size = 0.25, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:40.690746Z","iopub.execute_input":"2022-02-14T14:00:40.691043Z","iopub.status.idle":"2022-02-14T14:00:45.261170Z","shell.execute_reply.started":"2022-02-14T14:00:40.691002Z","shell.execute_reply":"2022-02-14T14:00:45.260261Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print('Training Features Shape:', train_x.shape)\nprint('Training Labels Shape:', train_y.shape)\nprint('Testing Features Shape:', test_x.shape)\nprint('Testing Labels Shape:', test_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:45.262505Z","iopub.execute_input":"2022-02-14T14:00:45.262807Z","iopub.status.idle":"2022-02-14T14:00:45.272938Z","shell.execute_reply.started":"2022-02-14T14:00:45.262768Z","shell.execute_reply":"2022-02-14T14:00:45.271999Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Train model - default Rand Forest\n\n**RandomForestClassifier**(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","metadata":{}},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf = RandomForestClassifier(random_state = 42, n_jobs=-1)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:00:45.276185Z","iopub.execute_input":"2022-02-14T14:00:45.276860Z","iopub.status.idle":"2022-02-14T14:01:02.342133Z","shell.execute_reply.started":"2022-02-14T14:00:45.276822Z","shell.execute_reply":"2022-02-14T14:01:02.341415Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions on test set","metadata":{}},{"cell_type":"code","source":"pred_y = clf.predict(test_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(test_y, pred_y))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:01:02.343283Z","iopub.execute_input":"2022-02-14T14:01:02.343521Z","iopub.status.idle":"2022-02-14T14:01:02.610581Z","shell.execute_reply.started":"2022-02-14T14:01:02.343491Z","shell.execute_reply":"2022-02-14T14:01:02.609634Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Train model - best params RandomSearch","metadata":{}},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf_rand = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='auto', \n                             max_depth=20, bootstrap=False, random_state = 42)\n\n# Train the model using the training sets y_pred=clf.predict(X_test)\nclf_rand.fit(train_x, train_y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions on test set","metadata":{}},{"cell_type":"code","source":"pred_y = clf_rand.predict(test_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy score:\", metrics.accuracy_score(test_y, pred_y))\n# Precision, Recall and Roc_AUC score\nprint(\"Precision score:\", metrics.precision_score(test_y, pred_y))\nprint(\"Recall score:\", metrics.recall_score(test_y, pred_y))\nprint(\"ROC_AUC score:\", metrics.roc_auc_score(test_y, pred_y))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(clf_rand, test_x, test_y)  \nplt.show()\n\n#plt.savefig('confmatrix_g1_uneven.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding important features\n\n1. Create a random forests model.\n2. Use the feature importance variable to see feature importance scores.\n3. Visualize these scores using the seaborn library.","metadata":{}},{"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(clf_rand.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 5)) for feature, importance in zip(feature_names, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp = pd.Series(clf_rand.feature_importances_, index = feature_names).sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = feature_imp.to_frame()\nfeatures.columns = ['Feature importance']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grouping all k-mers\ndimers = features[features.index.map(lambda x: \"2_mer_\" in x)]\ntrimers = features[features.index.map(lambda x: \"3_mer_\" in x)]\ntetramers = features[features.index.map(lambda x: \"4_mer_\" in x)]\npentamers = features[features.index.map(lambda x: \"5_mer_\" in x)] \nhexamers = features[features.index.map(lambda x: \"6_mer_\" in x)]\n\n# Grouping all aa-mers\nsingle_aa = features[features.index.map(lambda x: \"1_aa_mer_\" in x)]\ndouble_aa = features[features.index.map(lambda x: \"2_aa_mer_\" in x)]\ntriple_aa = features[features.index.map(lambda x: \"3_aa_mer_\" in x)]\n\n# Grouping c_weights\nc_weight = features[features.index.map(lambda x: \"c_weight\" in x)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mean feature importances of all k-mers, aa-mers and c_weights","metadata":{}},{"cell_type":"code","source":"data = {'Feature importance': [round(dimers.mean().iloc[0], 6), round(trimers.mean().iloc[0], 6), \n                               round(tetramers.mean().iloc[0], 6), round(pentamers.mean().iloc[0], 6), \n                               round(hexamers.mean().iloc[0], 6), round(single_aa.mean().iloc[0], 6), \n                               round(double_aa.mean().iloc[0], 6), round(triple_aa.mean().iloc[0], 6),\n                               round(c_weight.mean().iloc[0], 6)]}\n\nfeatures_cond = pd.DataFrame(data, index = ['dimers', 'trimers', 'tetramers', 'pentamers', 'hexamers', \n                                            'single_aa', 'double_aa', 'triple_aa', 'c_weight'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sum of all feature importances of k-mers, aa-mers and c_weights","metadata":{}},{"cell_type":"code","source":"data = {'Feature importance': [round(dimers.sum().iloc[0], 6), round(trimers.sum().iloc[0], 6), \n                               round(tetramers.sum().iloc[0], 6), round(pentamers.sum().iloc[0], 6), \n                               round(hexamers.sum().iloc[0], 6), round(single_aa.sum().iloc[0], 6), \n                               round(double_aa.sum().iloc[0], 6), round(triple_aa.sum().iloc[0], 6),\n                               round(c_weight.sum().iloc[0], 6)]}\n\nfeatures_cond2 = pd.DataFrame(data, index = ['dimers', 'trimers', 'tetramers', 'pentamers', 'hexamers', \n                                            'single_aa', 'double_aa', 'triple_aa', 'c_weight'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_cond","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_list = ['GC_content', 'GC1_content', 'GC2_content', 'GC3_content', \n            'Start_ATG', 'Start_GTG', 'Start_TTG', 'Length']\n\nfeatures_condensed = features.loc[ind_list]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_condensed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red = pd.concat([features_condensed, features_cond])\nfeatures_red = features_red.squeeze()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red = features_red.sort_values(ascending=False)\nfeatures_red","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red2 = pd.concat([features_condensed, features_cond2])\nfeatures_red2 = features_red2.squeeze()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red2 = features_red2.sort_values(ascending=False)\nfeatures_red2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x = features_red, y = features_red.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features (mean)\")\n#plt.legend(features_red.index, loc =\"lower right\", fontsize='x-small')\nplt.show()\n\n#plt.savefig('feature_imp_g1_agr_mean_uneven.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x = features_red2, y = features_red2.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features (sum)\")\n#plt.legend(features_red.index, loc =\"lower right\", fontsize='x-small')\nplt.show()\n\n#plt.savefig('feature_imp_g1_agr_uneven.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nfeatures_condensed = features_condensed.squeeze().sort_values(ascending=False)\n\n# Creating a bar plot\nsns.barplot(x = features_condensed, y = features_condensed.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.Series(clf_rand.feature_importances_, index = feature_names).sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected = feature_imp.iloc[0:15]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x = selected, y = selected.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\n#plt.legend()\nplt.show()\n\n#plt.savefig('feature_imp_g1_uneven.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating the model on selected features\n\nAfter removing the least important features the accuracy may increase. This is because one removes misleading data and noise, resulting in increased accuracy. A lesser amount of features also reduces the training time.","metadata":{}},{"cell_type":"code","source":"selected = feature_imp.iloc[0:200]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feat = list(selected.index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"important_indices = [feature_names.index(x) for x in selected_feat]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf_imp = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='auto', \n                             max_depth=20, bootstrap=False, random_state = 42)\n\n# Select most important features\nimportant_indices = [feature_names.index(x) for x in selected_feat]\n\ntrain_important = train_x[:, important_indices]\ntest_important = test_x[:, important_indices]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the random forest\nclf_imp.fit(train_important, train_y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions and determine the error\npred_y = clf_imp.predict(test_important)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy score:\", metrics.accuracy_score(test_y, pred_y))\n# Precision, Recall and Roc_AUC score\nprint(\"Precision score:\", metrics.precision_score(test_y, pred_y))\nprint(\"Recall score:\", metrics.recall_score(test_y, pred_y))\nprint(\"ROC_AUC score:\", metrics.roc_auc_score(test_y, pred_y))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion matrix","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(clf_imp, test_important, test_y)  \nplt.show()\n\n#plt.savefig('confmatrix_g1_bestmod_uneven.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After feature selection based on the most important features the metrics improved by 2%. In the confusion matrix we see that the clf_imp model generates much less false positives and slightly more false negatives. Meaning the accuracy, precision and ROC_AUC scores improve after feature selection and recall decreases slightly. ","metadata":{}},{"cell_type":"markdown","source":"## KLADD","metadata":{}},{"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(clf_rand.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_names, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Selecting only the 20 most important features\nfeature_importances = feature_importances[0:20]\nfeature_names = feature_names[0:20]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = feature_importances[0:20]\nfeature_names = feature_names[0:20]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n\n# List of x locations for plotting\nx_values = list(range(len(feature_importances)))\n\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","metadata":{},"execution_count":null,"outputs":[]}]}