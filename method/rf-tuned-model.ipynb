{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Random Forest Model - Tuned\n\nRF model to be used for predicting protein coding genes in DNA sequences.\n\n**Random Search with CV**\n- bootstrap': False\n- max_depth': 20\n- max_features': 'auto'\n- min_samples_leaf': 1\n- min_samples_split': 2\n- n_estimators': 1000\n \n**Grid Search with CV**\n- bootstrap': False\n- max_depth': 40\n- max_features': 3\n- min_samples_leaf': 1\n- min_samples_split': 2\n- n_estimators': 2000\n\nBEST MODEL G1 (high default score):\n- Random Search with CV - BUT\n- n_estimators': 200\n- n_features: 200\n\nBEST MODEL G2 (low default score):\n- Random search with CV\n- n_estimators': 1000\n- max_features': 'auto'\n- max_depth': 10\n- bootstrap': False","metadata":{}},{"cell_type":"code","source":"# Both genomes from E.coli species, but different strain\nG11 = pd.read_csv('../input/genomespart2/G11.features.csv').iloc[:, 1:]\nG12 = pd.read_csv('../input/genomespart2/G12.features.csv').iloc[:, 1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [G11, G12]\ndf = pd.concat(frames)\ndf.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G = df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Preprocessing and encoding variables\nimport pandas as pd\nimport numpy as np\n\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Visualising feature importance and making plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:34.381531Z","iopub.execute_input":"2022-04-26T14:30:34.381945Z","iopub.status.idle":"2022-04-26T14:30:34.389155Z","shell.execute_reply.started":"2022-04-26T14:30:34.381903Z","shell.execute_reply":"2022-04-26T14:30:34.388082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 genome at a time","metadata":{}},{"cell_type":"code","source":"G = pd.read_csv('../input/genomes-part1/G5.features.csv').iloc[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:34.671526Z","iopub.execute_input":"2022-04-26T14:30:34.672171Z","iopub.status.idle":"2022-04-26T14:30:49.990444Z","shell.execute_reply.started":"2022-04-26T14:30:34.672128Z","shell.execute_reply":"2022-04-26T14:30:49.989581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:49.992799Z","iopub.execute_input":"2022-04-26T14:30:49.993208Z","iopub.status.idle":"2022-04-26T14:30:50.023156Z","shell.execute_reply.started":"2022-04-26T14:30:49.993157Z","shell.execute_reply":"2022-04-26T14:30:50.021989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G = G.dropna()\n#G = G.drop(columns=['Length'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.024641Z","iopub.execute_input":"2022-04-26T14:30:50.024974Z","iopub.status.idle":"2022-04-26T14:30:50.339658Z","shell.execute_reply.started":"2022-04-26T14:30:50.02493Z","shell.execute_reply":"2022-04-26T14:30:50.33877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.341431Z","iopub.execute_input":"2022-04-26T14:30:50.341646Z","iopub.status.idle":"2022-04-26T14:30:50.370739Z","shell.execute_reply.started":"2022-04-26T14:30:50.341621Z","shell.execute_reply":"2022-04-26T14:30:50.369662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing / Data preparation\n\n1. One-hot encoded categorical variables\n2. Split data into features and labels\n3. Convert to arrays\n4. Split data into training and testing sets","metadata":{}},{"cell_type":"markdown","source":"### Encoding target values","metadata":{}},{"cell_type":"code","source":"def encode_feature(array):\n    \"\"\" Encode a categorical array into a number array\n    \n    :param array: array to be encoded\n    :return: numerical array\n    \"\"\"\n  \n    encoder = preprocessing.LabelEncoder()\n    encoder.fit(array)\n    return encoder.transform(array)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.372215Z","iopub.execute_input":"2022-04-26T14:30:50.372512Z","iopub.status.idle":"2022-04-26T14:30:50.378186Z","shell.execute_reply.started":"2022-04-26T14:30:50.37247Z","shell.execute_reply":"2022-04-26T14:30:50.377265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['CDS', 'LORF']\ntargets = G[\"Type\"].values\nprint(targets)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.379344Z","iopub.execute_input":"2022-04-26T14:30:50.379642Z","iopub.status.idle":"2022-04-26T14:30:50.398738Z","shell.execute_reply.started":"2022-04-26T14:30:50.379601Z","shell.execute_reply":"2022-04-26T14:30:50.39809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = encode_feature(targets)\nprint(targets)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.40006Z","iopub.execute_input":"2022-04-26T14:30:50.400272Z","iopub.status.idle":"2022-04-26T14:30:50.41284Z","shell.execute_reply.started":"2022-04-26T14:30:50.400247Z","shell.execute_reply":"2022-04-26T14:30:50.411866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The shape of our dataframe is:', G.shape)\nprint('Rows:', G.shape[0])\nprint('Columns:', G.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.413948Z","iopub.execute_input":"2022-04-26T14:30:50.414206Z","iopub.status.idle":"2022-04-26T14:30:50.431711Z","shell.execute_reply.started":"2022-04-26T14:30:50.414177Z","shell.execute_reply":"2022-04-26T14:30:50.430782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Selecting features and targets and converting data to arrays","metadata":{}},{"cell_type":"code","source":"# Labels are the values we want to predict\nlabels = targets\n\n# Remove the labels from the features -> axis 1 refers to the columns\nfeatures = G.drop(['Type','Genome', 'Dataset'], axis = 1)\n\n# Saving feature names as list for later use\nfeature_names = list(features.columns)\n\n# Convert to numpy array\nfeatures = np.array(features)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.433286Z","iopub.execute_input":"2022-04-26T14:30:50.434196Z","iopub.status.idle":"2022-04-26T14:30:50.895121Z","shell.execute_reply.started":"2022-04-26T14:30:50.434152Z","shell.execute_reply":"2022-04-26T14:30:50.894084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The shape of our features are:', features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.897901Z","iopub.execute_input":"2022-04-26T14:30:50.898151Z","iopub.status.idle":"2022-04-26T14:30:50.903786Z","shell.execute_reply.started":"2022-04-26T14:30:50.898122Z","shell.execute_reply":"2022-04-26T14:30:50.902834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split into training and testing sets","metadata":{}},{"cell_type":"code","source":"# Split the data into training and testing sets -> x = features and y = labels/targets\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size = 0.25, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:50.905331Z","iopub.execute_input":"2022-04-26T14:30:50.906238Z","iopub.status.idle":"2022-04-26T14:30:51.570612Z","shell.execute_reply.started":"2022-04-26T14:30:50.906168Z","shell.execute_reply":"2022-04-26T14:30:51.569564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Features Shape:', train_x.shape)\nprint('Training Labels Shape:', train_y.shape)\nprint('Testing Features Shape:', test_x.shape)\nprint('Testing Labels Shape:', test_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:51.571875Z","iopub.execute_input":"2022-04-26T14:30:51.572136Z","iopub.status.idle":"2022-04-26T14:30:51.578794Z","shell.execute_reply.started":"2022-04-26T14:30:51.572105Z","shell.execute_reply":"2022-04-26T14:30:51.577754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model - default Rand Forest\n\n**RandomForestClassifier**(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","metadata":{}},{"cell_type":"code","source":"# Create a Gaussian Classifier\n#clf = RandomForestClassifier(random_state = 42, n_jobs=-1)\n\n# Train the model using the training sets\n#clf.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:51.580184Z","iopub.execute_input":"2022-04-26T14:30:51.580906Z","iopub.status.idle":"2022-04-26T14:30:51.590168Z","shell.execute_reply.started":"2022-04-26T14:30:51.580867Z","shell.execute_reply":"2022-04-26T14:30:51.58922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions on test set","metadata":{}},{"cell_type":"code","source":"#pred_y = clf.predict(test_x)\n\n# Model Accuracy, how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(test_y, pred_y))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:51.591636Z","iopub.execute_input":"2022-04-26T14:30:51.591876Z","iopub.status.idle":"2022-04-26T14:30:51.604289Z","shell.execute_reply.started":"2022-04-26T14:30:51.591837Z","shell.execute_reply":"2022-04-26T14:30:51.603478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model - best params RandomSearch","metadata":{}},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf_rand = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='auto', \n                             max_depth=20, bootstrap=False, random_state = 42)\n\n# Train the model using the training sets\nclf_rand.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:30:51.60559Z","iopub.execute_input":"2022-04-26T14:30:51.605828Z","iopub.status.idle":"2022-04-26T14:31:28.371635Z","shell.execute_reply.started":"2022-04-26T14:30:51.605802Z","shell.execute_reply":"2022-04-26T14:31:28.370751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions on test set","metadata":{}},{"cell_type":"code","source":"pred_y = clf_rand.predict(test_x)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy score:\", metrics.accuracy_score(test_y, pred_y))\n# Precision, Recall and Roc_AUC score\n#print(\"Precision score:\", metrics.precision_score(test_y, pred_y))\n#print(\"Recall score:\", metrics.recall_score(test_y, pred_y))\n#print(\"ROC_AUC score:\", metrics.roc_auc_score(test_y, pred_y))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.372885Z","iopub.execute_input":"2022-04-26T14:31:28.373335Z","iopub.status.idle":"2022-04-26T14:31:28.506106Z","shell.execute_reply.started":"2022-04-26T14:31:28.373304Z","shell.execute_reply":"2022-04-26T14:31:28.505184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(clf_rand, test_x, test_y)  \nplt.show()\n\n#plt.savefig('confmatrix_g1_uneven.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.507585Z","iopub.execute_input":"2022-04-26T14:31:28.507854Z","iopub.status.idle":"2022-04-26T14:31:28.831801Z","shell.execute_reply.started":"2022-04-26T14:31:28.507783Z","shell.execute_reply":"2022-04-26T14:31:28.830913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding important features\n\n1. Create a random forests model.\n2. Use the feature importance variable to see feature importance scores.\n3. Visualize these scores using the seaborn library.","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.Series(clf_rand.feature_importances_, index = feature_names).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.832973Z","iopub.execute_input":"2022-04-26T14:31:28.833191Z","iopub.status.idle":"2022-04-26T14:31:28.877397Z","shell.execute_reply.started":"2022-04-26T14:31:28.833166Z","shell.execute_reply":"2022-04-26T14:31:28.876411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.879031Z","iopub.execute_input":"2022-04-26T14:31:28.879327Z","iopub.status.idle":"2022-04-26T14:31:28.889243Z","shell.execute_reply.started":"2022-04-26T14:31:28.879284Z","shell.execute_reply":"2022-04-26T14:31:28.888377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = feature_imp.to_frame()\nfeatures.columns = ['Feature importance']","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.890866Z","iopub.execute_input":"2022-04-26T14:31:28.891776Z","iopub.status.idle":"2022-04-26T14:31:28.899794Z","shell.execute_reply.started":"2022-04-26T14:31:28.891728Z","shell.execute_reply":"2022-04-26T14:31:28.899192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.900957Z","iopub.execute_input":"2022-04-26T14:31:28.90176Z","iopub.status.idle":"2022-04-26T14:31:28.920884Z","shell.execute_reply.started":"2022-04-26T14:31:28.901712Z","shell.execute_reply":"2022-04-26T14:31:28.920064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grouping all k-mers\ndimers = features[features.index.map(lambda x: \"2_mer_\" in x)]\ntrimers = features[features.index.map(lambda x: \"3_mer_\" in x)]\ntetramers = features[features.index.map(lambda x: \"4_mer_\" in x)]\npentamers = features[features.index.map(lambda x: \"5_mer_\" in x)] \nhexamers = features[features.index.map(lambda x: \"6_mer_\" in x)]\n\n# Grouping all aa-mers\nsingle_aa = features[features.index.map(lambda x: \"1_aa_mer_\" in x)]\ndouble_aa = features[features.index.map(lambda x: \"2_aa_mer_\" in x)]\ntriple_aa = features[features.index.map(lambda x: \"3_aa_mer_\" in x)]\n\n# Grouping c_weights\nc_weight = features[features.index.map(lambda x: \"c_weight\" in x)]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.922086Z","iopub.execute_input":"2022-04-26T14:31:28.922823Z","iopub.status.idle":"2022-04-26T14:31:28.978072Z","shell.execute_reply.started":"2022-04-26T14:31:28.922785Z","shell.execute_reply":"2022-04-26T14:31:28.977298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mean feature importances of all k-mers, aa-mers and c_weights","metadata":{}},{"cell_type":"code","source":"data = {'Feature importance': [round(dimers.mean().iloc[0], 6), round(trimers.mean().iloc[0], 6), \n                               round(tetramers.mean().iloc[0], 6), round(pentamers.mean().iloc[0], 6), \n                               round(hexamers.mean().iloc[0], 6), round(single_aa.mean().iloc[0], 6), \n                               round(double_aa.mean().iloc[0], 6), round(triple_aa.mean().iloc[0], 6),\n                               round(c_weight.mean().iloc[0], 6)]}\n\nfeatures_cond = pd.DataFrame(data, index = ['dimers', 'trimers', 'tetramers', 'pentamers', 'hexamers', \n                                            'single_aa', 'double_aa', 'triple_aa', 'c_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.981648Z","iopub.execute_input":"2022-04-26T14:31:28.982542Z","iopub.status.idle":"2022-04-26T14:31:28.995297Z","shell.execute_reply.started":"2022-04-26T14:31:28.982483Z","shell.execute_reply":"2022-04-26T14:31:28.994448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sum of all feature importances of k-mers, aa-mers and c_weights","metadata":{}},{"cell_type":"code","source":"data = {'Feature importance': [round(dimers.sum().iloc[0], 6), round(trimers.sum().iloc[0], 6), \n                               round(tetramers.sum().iloc[0], 6), round(pentamers.sum().iloc[0], 6), \n                               round(hexamers.sum().iloc[0], 6), round(single_aa.sum().iloc[0], 6), \n                               round(double_aa.sum().iloc[0], 6), round(triple_aa.sum().iloc[0], 6),\n                               round(c_weight.sum().iloc[0], 6)]}\n\nfeatures_cond2 = pd.DataFrame(data, index = ['dimers', 'trimers', 'tetramers', 'pentamers', 'hexamers', \n                                            'single_aa', 'double_aa', 'triple_aa', 'c_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:28.996556Z","iopub.execute_input":"2022-04-26T14:31:28.996801Z","iopub.status.idle":"2022-04-26T14:31:29.020761Z","shell.execute_reply.started":"2022-04-26T14:31:28.99677Z","shell.execute_reply":"2022-04-26T14:31:29.0198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_cond","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.022303Z","iopub.execute_input":"2022-04-26T14:31:29.022532Z","iopub.status.idle":"2022-04-26T14:31:29.038878Z","shell.execute_reply.started":"2022-04-26T14:31:29.022498Z","shell.execute_reply":"2022-04-26T14:31:29.038041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_list = ['GC_content', 'GC1_content', 'GC2_content', 'GC3_content', \n            'Start_ATG', 'Start_GTG', 'Start_TTG', 'Length']\n\nfeatures_condensed = features.loc[ind_list]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.040495Z","iopub.execute_input":"2022-04-26T14:31:29.041254Z","iopub.status.idle":"2022-04-26T14:31:29.054544Z","shell.execute_reply.started":"2022-04-26T14:31:29.041197Z","shell.execute_reply":"2022-04-26T14:31:29.053743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_condensed","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.055543Z","iopub.execute_input":"2022-04-26T14:31:29.056373Z","iopub.status.idle":"2022-04-26T14:31:29.072435Z","shell.execute_reply.started":"2022-04-26T14:31:29.056324Z","shell.execute_reply":"2022-04-26T14:31:29.071365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red = pd.concat([features_condensed, features_cond])\nfeatures_red = features_red.squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.076304Z","iopub.execute_input":"2022-04-26T14:31:29.077165Z","iopub.status.idle":"2022-04-26T14:31:29.082455Z","shell.execute_reply.started":"2022-04-26T14:31:29.07712Z","shell.execute_reply":"2022-04-26T14:31:29.081599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red = features_red.sort_values(ascending=False)\nfeatures_red","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.084088Z","iopub.execute_input":"2022-04-26T14:31:29.084651Z","iopub.status.idle":"2022-04-26T14:31:29.096732Z","shell.execute_reply.started":"2022-04-26T14:31:29.084606Z","shell.execute_reply":"2022-04-26T14:31:29.095845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red2 = pd.concat([features_condensed, features_cond2])\nfeatures_red2 = features_red2.squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.098196Z","iopub.execute_input":"2022-04-26T14:31:29.099268Z","iopub.status.idle":"2022-04-26T14:31:29.108542Z","shell.execute_reply.started":"2022-04-26T14:31:29.099224Z","shell.execute_reply":"2022-04-26T14:31:29.10788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red2 = features_red2.sort_values(ascending=False)\nfeatures_red2","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.109461Z","iopub.execute_input":"2022-04-26T14:31:29.110307Z","iopub.status.idle":"2022-04-26T14:31:29.125068Z","shell.execute_reply.started":"2022-04-26T14:31:29.11026Z","shell.execute_reply":"2022-04-26T14:31:29.124211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x = features_red, y = features_red.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features (mean)\")\n#plt.savefig('feat_imp_ecoli_1.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.126177Z","iopub.execute_input":"2022-04-26T14:31:29.126733Z","iopub.status.idle":"2022-04-26T14:31:29.406615Z","shell.execute_reply.started":"2022-04-26T14:31:29.126687Z","shell.execute_reply":"2022-04-26T14:31:29.405597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x = features_red2, y = features_red2.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features (sum)\")\n#plt.savefig('feat_imp_ecoli_2.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.407968Z","iopub.execute_input":"2022-04-26T14:31:29.408224Z","iopub.status.idle":"2022-04-26T14:31:29.676132Z","shell.execute_reply.started":"2022-04-26T14:31:29.408195Z","shell.execute_reply":"2022-04-26T14:31:29.675062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nfeatures_condensed = features_condensed.squeeze().sort_values(ascending=False)\n\n# Creating a bar plot\nsns.barplot(x = features_condensed, y = features_condensed.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.677581Z","iopub.execute_input":"2022-04-26T14:31:29.677821Z","iopub.status.idle":"2022-04-26T14:31:29.890322Z","shell.execute_reply.started":"2022-04-26T14:31:29.677793Z","shell.execute_reply":"2022-04-26T14:31:29.889191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.Series(clf_rand.feature_importances_, index = feature_names).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.891515Z","iopub.execute_input":"2022-04-26T14:31:29.891859Z","iopub.status.idle":"2022-04-26T14:31:29.9403Z","shell.execute_reply.started":"2022-04-26T14:31:29.891813Z","shell.execute_reply":"2022-04-26T14:31:29.939241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected = feature_imp.iloc[0:15]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.941568Z","iopub.execute_input":"2022-04-26T14:31:29.941825Z","iopub.status.idle":"2022-04-26T14:31:29.947337Z","shell.execute_reply.started":"2022-04-26T14:31:29.941792Z","shell.execute_reply":"2022-04-26T14:31:29.946486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Creating a bar plot\nsns.barplot(x = selected, y = selected.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\n#plt.savefig('feat_imp_ecoli_3.png',dpi=300, bbox_inches='tight')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:29.948545Z","iopub.execute_input":"2022-04-26T14:31:29.949443Z","iopub.status.idle":"2022-04-26T14:31:30.215611Z","shell.execute_reply.started":"2022-04-26T14:31:29.949398Z","shell.execute_reply":"2022-04-26T14:31:30.214908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating the model on selected features\n\nAfter removing the least important features the accuracy may increase. This is because one removes misleading data and noise, resulting in increased accuracy. A lesser amount of features also reduces the training time.\n\nExtra features can decrease performance because they may “confuse” the model by giving it irrelevant data that prevents it from learning the actual relationships. The random forest performs implicit feature selection because it splits nodes on the most important variables, but other machine learning models do not. One approach to improve other models is therefore to use the random forest feature importances to reduce the number of variables in the problem. In our case, we will use the feature importances to decrease the number of features for our random forest model, because, in addition to potentially increasing performance, reducing the number of features will shorten the run time of the model. \n\nOften with feature reduction, there will be a minor decrease in performance that must be weighed against the decrease in run-time. Machine learning is a game of making trade-offs, and run-time versus performance is usually one of the critical decisions.","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.Series(clf_rand.feature_importances_, index = feature_names).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:30.21671Z","iopub.execute_input":"2022-04-26T14:31:30.217055Z","iopub.status.idle":"2022-04-26T14:31:30.262411Z","shell.execute_reply.started":"2022-04-26T14:31:30.217024Z","shell.execute_reply":"2022-04-26T14:31:30.261621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected = feature_imp.iloc[0:300]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:30.26351Z","iopub.execute_input":"2022-04-26T14:31:30.263865Z","iopub.status.idle":"2022-04-26T14:31:30.267981Z","shell.execute_reply.started":"2022-04-26T14:31:30.263836Z","shell.execute_reply":"2022-04-26T14:31:30.267077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feat = list(selected.index)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:30.269604Z","iopub.execute_input":"2022-04-26T14:31:30.269912Z","iopub.status.idle":"2022-04-26T14:31:30.280963Z","shell.execute_reply.started":"2022-04-26T14:31:30.269872Z","shell.execute_reply":"2022-04-26T14:31:30.280108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"important_indices = [feature_names.index(x) for x in selected_feat]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:30.282468Z","iopub.execute_input":"2022-04-26T14:31:30.282814Z","iopub.status.idle":"2022-04-26T14:31:30.310691Z","shell.execute_reply.started":"2022-04-26T14:31:30.282786Z","shell.execute_reply":"2022-04-26T14:31:30.309825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Gaussian Classifier\nclf_imp = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='auto', \n                             max_depth=10, bootstrap=False, random_state = 42)\n\n# Select most important features\nimportant_indices = [feature_names.index(x) for x in selected_feat]\n\ntrain_important = train_x[:, important_indices]\ntest_important = test_x[:, important_indices]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:30.312105Z","iopub.execute_input":"2022-04-26T14:31:30.31274Z","iopub.status.idle":"2022-04-26T14:31:30.351758Z","shell.execute_reply.started":"2022-04-26T14:31:30.312707Z","shell.execute_reply":"2022-04-26T14:31:30.351084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the random forest\nclf_imp.fit(train_important, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:30.353373Z","iopub.execute_input":"2022-04-26T14:31:30.353983Z","iopub.status.idle":"2022-04-26T14:31:36.125614Z","shell.execute_reply.started":"2022-04-26T14:31:30.353939Z","shell.execute_reply":"2022-04-26T14:31:36.124744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions and determine the error\npred_y = clf_imp.predict(test_important)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy score:\", metrics.accuracy_score(test_y, pred_y))\n# Precision, Recall and Roc_AUC score\nprint(\"Precision score:\", metrics.precision_score(test_y, pred_y))\nprint(\"Recall score:\", metrics.recall_score(test_y, pred_y))\n#print(\"ROC_AUC score:\", metrics.roc_auc_score(test_y, pred_y))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:36.12727Z","iopub.execute_input":"2022-04-26T14:31:36.127829Z","iopub.status.idle":"2022-04-26T14:31:36.189574Z","shell.execute_reply.started":"2022-04-26T14:31:36.127783Z","shell.execute_reply":"2022-04-26T14:31:36.188714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion matrix","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(clf_imp, test_important, test_y)  \nplt.show()\n\nplt.savefig('confmatrix_g5.png', dpi=300, bbox_inches='tight', transparent=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:31:36.191326Z","iopub.execute_input":"2022-04-26T14:31:36.192022Z","iopub.status.idle":"2022-04-26T14:31:36.447881Z","shell.execute_reply.started":"2022-04-26T14:31:36.19195Z","shell.execute_reply":"2022-04-26T14:31:36.447017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After feature selection based on the most important features the metrics improved by 2%. In the confusion matrix we see that the clf_imp model generates much less false positives and slightly more false negatives. Meaning the accuracy, precision and ROC_AUC scores improve after feature selection and recall decreases slightly. ","metadata":{}},{"cell_type":"markdown","source":"## KLADD","metadata":{}},{"cell_type":"markdown","source":"### Separate out unwanted data based on column 'Dataset'","metadata":{}},{"cell_type":"code","source":"G = G.loc[G['Dataset'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The shape of our dataframe is:', G.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}